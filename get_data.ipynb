{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD:proyecto_ingC.ipynb
   "execution_count": 10,
=======
   "execution_count": 44,
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
<<<<<<< HEAD:proyecto_ingC.ipynb
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
=======
    "import re\n",
    "from datetime import datetime "
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping IMSSS\n",
    "### Web Scrapping de la página general por años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardamos el url base para poder concatenar los links de los archivos\n",
    "base_url = \"http://datos.imss.gob.mx\"\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(\"http://datos.imss.gob.mx/dataset\")\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\") \n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", {'data-format':'csv'}, href=True)\n",
    "# Se guardaran los links de los archivos que nos interesan por año\n",
    "links = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for link in spans:\n",
    "    href = link.get(\"href\")\n",
    "    if '/dataset/asg-' in href:\n",
    "        condicion = int(re.findall(r'\\d{4}', href)[0])\n",
    "        if condicion >= 2018 and condicion < 2023:\n",
    "            links.append(base_url + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:proyecto_ingC.ipynb
    "### Web Scrapping parte 2, por meses"
=======
    "#### Descripción de fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea y abre el archivo donde se excribe la información de la referncia\n",
    "file = open('descripciones\\imss_descripcion.txt', 'w+',encoding='utf-8')\n",
    "file.write('Datos del IMSS\\n')\n",
    "for link in links:\n",
    "    r = requests.get(link)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    file.write('Enlace de archivos por mes: '+link+'\\n')\n",
    "    # Tabla de referencias\n",
    "    table = soup.find('table', class_= 'field-group-format group_additional')\n",
    "    # Moviendose por filas\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        if row:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            # Extrae el texto de las celdas y los concatena\n",
    "            file.write(': '.join(cell.get_text(strip=True) for cell in cells))\n",
    "            file.write('\\n')\n",
    "# Agrega la fecha en la que se corrió el script que extrajo los datos\n",
    "fecha_actual = datetime.now().strftime('%Y-%m-%d')\n",
    "file.write('\\nFecha de descarga: '+fecha_actual+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scrapping pt 2 por meses"
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Por cada año, guardaremos los links de los archivos que nos interesan por meses\n",
    "links_2 = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links)):\n",
    "    r_2 = requests.get(links[i])\n",
    "    soup_2 = BeautifulSoup(r_2.content, \"html.parser\")\n",
    "    spans_2 = soup_2.find_all(\"a\", {'property':'dcat:accessURL'}, href=True)\n",
    "    for link in spans_2:\n",
    "        links_2.append(base_url + link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una lista para guardar los dataframes\n",
    "df = []\n",
    "# Obtenemos los links de los archivos que nos interesan\n",
    "for i in range(len(links_2)):\n",
    "    r_3 = requests.get(links_2[i])\n",
    "    soup_3 = BeautifulSoup(r_3.content, \"html.parser\")\n",
    "    spans_3 = soup_3.find_all(\"a\", href=True)\n",
    "    for link in spans_3:\n",
    "        if '.csv' in link.get(\"href\"):\n",
    "            df.append(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame vacío para almacenar los datos combinados\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\n",
    "    df_csv = pd.read_csv(df[i], encoding=\"latin-1\", sep=\"|\", dtype={\"columna_con_tipo_mezclado\": str}, low_memory=False)\n",
    "\n",
    "    # Columnas que no se van a usar\n",
    "    columns = ['rango_uma', 'ta', 'teu', 'tec', 'tpu', 'tpc',\n",
    "               'ta_sal', 'teu_sal', 'tec_sal', 'tpu_sal', 'tpc_sal', 'masa_sal_ta',\n",
    "               'masa_sal_teu', 'masa_sal_tec', 'masa_sal_tpu', 'masa_sal_tpc']\n",
    "\n",
    "    # Usar copy() para evitar SettingWithCopyWarning\n",
    "    df_sonora = df_csv.loc[(df_csv[\"sector_economico_1\"] == 0) & (df_csv[\"cve_entidad\"] == 26)].copy()\n",
    "\n",
    "    # Eliminar columnas\n",
    "    df_sonora.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "    # Extraer año y mes de la URL\n",
    "    df_sonora['año'] = re.findall(r'\\d{4}', df[i])[0]\n",
    "    df_sonora['mes'] = re.findall(r'-(\\d{2})-', df[i])[0]\n",
    "\n",
    "    # Filtrar por sector_economico_2\n",
    "    df_append = df_sonora[df_sonora[\"sector_economico_2\"].isin([1, 2, 4])]\n",
    "\n",
    "    # Usar pd.concat en lugar de append (sin ignore_index=True)\n",
    "    df_combined = pd.concat([df_combined, df_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "df_combined.to_csv('data\\df_imss.csv') \n",
    "# files.download('df_imss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:proyecto_ingC.ipynb
    "# Leer el archivo CSV y especificar el tipo de datos adecuado (si es necesario)\n",
    "df_imss = pd.read_csv('df_imss.csv')"
=======
    "df_imss = pd.read_csv('data\\df_imss.csv')"
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion para catalogos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalogos(tipo_catalogo, soup):\n",
    "    # Utilizar una expresión regular para buscar elementos <a> con 'title' que contiene 'Catalogos'\n",
    "    pattern = re.compile(r'Cat[aá]logos', re.IGNORECASE)  # La opción re.IGNORECASE hace que la búsqueda sea insensible a mayúsculas/minúsculas\n",
    "    enlace = soup.find_all(\"a\", {'title':pattern}, href=True)\n",
    "    for link in enlace:\n",
    "        href = link.get(\"href\")\n",
    "        df = pd.read_excel(href, sheet_name=None)\n",
    "        # Iteramos a través de las hojas del archivo\n",
    "        for sheet_name, sheet_data in df.items():\n",
    "            # Genera un nombre de archivo CSV basado en el nombre de la hoja\n",
    "            nombre_archivo_csv = f\"{tipo_catalogo}_{sheet_name}.csv\"\n",
    "            # Guarda el DataFrame de la hoja en un archivo CSV\n",
    "            sheet_data.to_csv(nombre_archivo_csv, index=False)\n",
    "            print(f\"La hoja '{sheet_name}' se ha guardado en '{nombre_archivo_csv}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Agricultura (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:proyecto_ingC.ipynb
   "execution_count": 27,
=======
   "execution_count": 73,
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'ClasificaciónCultivos' se ha guardado en 'Agricultura_ClasificaciónCultivos.csv'\n",
      "La hoja 'CiclosCultivos' se ha guardado en 'Agricultura_CiclosCultivos.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Agricultura_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Agricultura_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Agricultura_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el url\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-de-agricultura-sonora/1573'\n",
    "# Obtenemos el contenido de la página\n",
    "r = requests.get(url)\n",
    "# Creamos el objeto soup\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "# Obtenemos todos los links de la página\n",
    "spans = soup.find_all(\"a\", href=True)\n",
<<<<<<< HEAD:proyecto_ingC.ipynb
=======
    "\n",
    "# Información de la fuente\n",
    "file = open('descripciones\\\\agricultura_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "\n",
    "# Llamamos a la función catalogos\n",
    "catalogos('Agricultura', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de enlaces a archivos xlsx\n",
    "links_xlsx = []\n",
    "# Recorre la lista de años y busca enlaces con títulos que coincidan con cada año\n",
    "for year in years:\n",
    "    links = soup.find_all(\"a\", {'title': f'Producción agrícola {year}'}, href=True)\n",
    "    for link in links:\n",
    "        links_xlsx.append(link['href'])\n",
    "        \n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_agricultura = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_agricultura = pd.concat([df_agricultura, pd.read_excel(links_xlsx[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agricultura.to_csv('data\\df_agricultura.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pecuaria (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:proyecto_ingC.ipynb
   "execution_count": 31,
=======
   "execution_count": 74,
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Catálogo_Ganadería' se ha guardado en 'Pecuaria_Catálogo_Ganadería.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Pecuaria_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Pecuaria_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Pecuaria_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-ganaderia-sonora/1581'\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
<<<<<<< HEAD:proyecto_ingC.ipynb
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
=======
    "# Información de la fuente\n",
    "file = open('descripciones\\\\pecuaria_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "\n",
    "catalogos('Pecuaria', soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pecuaria = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pecuaria = pd.concat([df_pecuaria, pd.read_excel(links_xlsx[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pecuaria.to_csv('data\\df_pecuaria.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Pesca (2018-2022)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:proyecto_ingC.ipynb
   "execution_count": 32,
=======
   "execution_count": 75,
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La hoja 'Especie_Pesca' se ha guardado en 'Pesca_Especie_Pesca.csv'\n",
      "La hoja 'Distritos' se ha guardado en 'Pesca_Distritos.csv'\n",
      "La hoja 'Municipios' se ha guardado en 'Pesca_Municipios.csv'\n",
      "La hoja 'Regiones' se ha guardado en 'Pesca_Regiones.csv'\n"
     ]
    }
   ],
   "source": [
    "# URL de la página HTML\n",
    "url = 'https://datos.sonora.gob.mx/conjuntos-de-datos/mostrar/datos-pesca-sonora/1582'\n",
    "# Realiza la solicitud HTTP\n",
    "r = requests.get(url)\n",
    "# Parsea el contenido HTML\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
<<<<<<< HEAD:proyecto_ingC.ipynb
=======
    "# Información de la fuente\n",
    "file = open('descripciones\\\\pesca_descripcion.txt', 'w+', encoding = 'utf-8')\n",
    "file.write('Enlace de los archivos: '+url+'\\n')\n",
    "data_info = soup.find('p', class_='mb-3').get_text()\n",
    "info = data_info.split(' | ')\n",
    "file.write('\\n'.join(info))\n",
    "file.write('\\nFecha de descarga: '+fecha_actual)\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
    "# Lista de años\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "# Expresión regular para buscar enlaces .xlsx\n",
    "xlsx_pattern = re.compile(r'\\.xlsx$')\n",
    "# Busca todos los enlaces con extensión .xlsx en la página\n",
    "enlaces = soup.find_all('a', href=xlsx_pattern)\n",
    "\n",
    "# Llamamos a la función catalogos\n",
    "catalogos('Pesca', soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar los enlaces .xlsx\n",
    "links_xlsx = []\n",
    "# Recorre los enlaces y verifica si el texto del enlace contiene un año de la lista\n",
    "for enlace in enlaces:\n",
    "    texto_enlace = enlace.text\n",
    "    for year in years:\n",
    "        if year in texto_enlace:\n",
    "            links_xlsx.append(enlace['href'])\n",
    "\n",
    "# Elimina duplicados (si los hay) utilizando set y luego convierte de nuevo a lista\n",
    "links_xlsx = list(set(links_xlsx))\n",
    "\n",
    "# Creamos un DataFrame vacío para almacenar los datos combinados\n",
    "df_pesca = pd.DataFrame()\n",
    "for i in range(len(links_xlsx)):\n",
    "    df_pesca = pd.concat([df_pesca, pd.read_excel(links_xlsx[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD:proyecto_ingC.ipynb
    "# Guardamos el DataFrame en un archivo CSV\n",
    "df_pesca.to_csv('df_pesca.csv')"
=======
    "df_pesca.to_csv('data\\df_pesca.csv')"
>>>>>>> a226a261bb726796ffa0a225bb6b4f9753876a67:get_data.ipynb
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
